{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3447df4c",
   "metadata": {},
   "source": [
    "# Notebook for evaluate tempering performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d617c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_util import *\n",
    "import torch\n",
    "from energy.a4 import A4, A6, AldpBoltzmann\n",
    "from network.egnn import remove_mean\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mdtraj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#                ✨ Hyperparameters ✨\n",
    "# ============================================================\n",
    "\n",
    "target_name = 'a4'\n",
    "sample_path = ''\n",
    "gt_sample_path = '6A_trajectory_1.0_600.0.h5'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95ab5a",
   "metadata": {},
   "source": [
    "### Define targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e920ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_name == 'a4':\n",
    "    n_particles = 43\n",
    "    target = A4(500, 'cuda', scaling=1.0)\n",
    "elif target_name == 'a6':\n",
    "    n_particles = 63\n",
    "    target = A6(600, 'cuda', scaling=1.0)\n",
    "elif target_name == 'aldp':\n",
    "    n_particles = 22\n",
    "    target = AldpBoltzmann(300, 'cuda')\n",
    "else:\n",
    "    raise ValueError(f\"Unknown target: {target_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0480ff9e",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed515b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ground truth data and calculate histograms\n",
    "\n",
    "data_low = torch.from_numpy(remove_mean(mdtraj.load(gt_sample_path).xyz, n_particles, 3)).to(device).reshape(-1, n_particles*3) \n",
    "data_low = data_low.reshape(-1, n_particles, 3) - data_low.reshape(-1, n_particles, 3).mean(dim=1, keepdim=True)\n",
    "data_low = data_low.reshape(-1, n_particles*3)\n",
    "reference = data_low[0].clone()\n",
    "# superimpose the data\n",
    "data_low = superimpose_B_onto_A(reference.reshape(n_particles, 3), data_low.reshape(-1, n_particles, 3), np.arange(43))\n",
    "data_low = data_low.reshape(-1, n_particles*3)\n",
    "\n",
    "log_p = target.log_prob(data_low[np.random.choice(data_low.shape[0], 25000, replace=False)].reshape(-1, 43*3))\n",
    "hist, bins = np.histogram(log_p.cpu().numpy(), bins=100, density=True)\n",
    "hist = hist / np.sum(hist)\n",
    "# get distance\n",
    "def get_dist(x):\n",
    "    x = (((x.reshape(-1, n_particles, 1, 3) - x.reshape(-1, 1, n_particles, 3))**2).sum(-1).sqrt()).cpu()\n",
    "    diagx = torch.triu_indices(x.shape[1], x.shape[1], 1)\n",
    "    return x[:, diagx[0], diagx[1]].flatten()\n",
    "\n",
    "dist = get_dist(data_low[np.random.choice(data_low.shape[0], 50*500, replace=False)])\n",
    "dist_hist, dist_bins = np.histogram(dist.cpu().numpy(), bins=100, density=True)\n",
    "dist_hist = dist_hist / np.sum(dist_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57af9d4",
   "metadata": {},
   "source": [
    "### Load Tempering Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ff267",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = np.load(sample_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ae2fe2",
   "metadata": {},
   "source": [
    "### Set TICA Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tica = torch.from_numpy(remove_mean(mdtraj.load(gt_sample_path).xyz, n_particles, 3)).to(device).reshape(-1, n_particles, 3)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import mdtraj as md\n",
    "import pyemma\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Build an mdtraj.Trajectory from your tensor\n",
    "xyz = data_tica.detach().cpu().numpy()            # (T, N, 3)\n",
    "top = md.Topology.from_openmm(target.system.topology)\n",
    "traj = md.Trajectory(xyz, topology=top)\n",
    "\n",
    "# Backbone torsions (ϕ, ψ) with cosine/sine to handle periodicity\n",
    "phi_idx,  phi = md.compute_phi(traj)    # radians, shape (T, n_phi)\n",
    "psi_idx,  psi = md.compute_psi(traj)    # radians, shape (T, n_psi)\n",
    "feat_tors = np.hstack([phi % (np.pi*2), psi % (np.pi*2)]) \n",
    "\n",
    "# TICA on your feature matrix\n",
    "tica = pyemma.coordinates.tica([feat_tors], lag=8, dim=2)\n",
    "Y = tica.transform(feat_tors)  # (T, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b362e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmd import MMD_loss\n",
    "\n",
    "# use GT data to set the kernel width of MMD\n",
    "sigma_2 = ((Y[None, ::100] - Y[::100, None])**2).sum(-1)**0.5\n",
    "sigma_2 = np.median(sigma_2.flatten())**2\n",
    "MMD_fnc = MMD_loss(2, 10, sigma_2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c037d2",
   "metadata": {},
   "source": [
    "### Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dfc42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Energy_TVD = []\n",
    "Distance_TVD = []\n",
    "Sample_W2 = []\n",
    "TICA_MMD = []\n",
    "\n",
    "for _ in range(3): # repeat 3 times, each time eval with 5000 samples\n",
    "    budget = 50000 # this is to aligh with the inference time scaling budget of RNC\n",
    "    samples = torch.from_numpy(all_samples)[1000:budget].to(device).reshape(-1, n_particles*3) \n",
    "    original_samples = samples.clone()\n",
    "    # thin it to 5000 samples\n",
    "    samples = samples[np.random.choice(samples.shape[0], 5000, replace=False)]\n",
    "\n",
    "\n",
    "    # evaluate the log density of the thin samples\n",
    "    thin_log_p = target.log_prob(data_low[np.random.choice(data_low.shape[0], 5000, replace=False)].reshape(-1, n_particles*3))\n",
    "    thin_hist, _ = np.histogram(thin_log_p.cpu().numpy(), bins=bins, density=True)\n",
    "    thin_hist = thin_hist / np.sum(thin_hist)\n",
    "\n",
    "    # calculate Energy TVD\n",
    "    log_p_s = target.log_prob(samples)\n",
    "    s_hist, s_bins = np.histogram(log_p_s.cpu().numpy(), bins=bins, density=True)\n",
    "    s_hist = s_hist / np.sum(s_hist)\n",
    "\n",
    "    TVD = thin_hist - s_hist\n",
    "    TVD = np.abs(TVD)\n",
    "    TVD = np.sum(TVD) / 2\n",
    "    print(f\"Energy TVD: {TVD}\")\n",
    "\n",
    "    Energy_TVD.append(TVD)\n",
    "\n",
    "\n",
    "\n",
    "    # remove mean\n",
    "    samples = samples.reshape(-1, n_particles, 3) - samples.reshape(-1, n_particles, 3).mean(dim=1, keepdim=True)\n",
    "    samples = samples.reshape(-1, n_particles*3)\n",
    "    # align\n",
    "    samples = superimpose_B_onto_A(reference.reshape(n_particles, 3), samples.reshape(-1, n_particles, 3), np.arange(n_particles))\n",
    "    samples = samples.reshape(-1, n_particles*3)\n",
    "\n",
    "\n",
    "    s1 = samples[np.random.choice(samples.shape[0], 5000, replace=False), None].reshape(-1, n_particles, 3).cpu() \n",
    "    s2 = data_low[np.random.choice(data_low.shape[0], 5000, replace=False), None].reshape(-1, n_particles, 3).cpu()\n",
    "\n",
    "    # calculate Sample W2\n",
    "    w2 = compute_distribution_distances(s1.reshape(-1, n_particles*3)[:, None], s2.reshape(-1, n_particles*3)[:, None])\n",
    "    w2  = w2[1][1]\n",
    "    print(f\"Sample W2: {w2}\")\n",
    "    Sample_W2.append(w2)\n",
    "\n",
    "\n",
    "    # calculate Distance TVD\n",
    "    dist_samples = get_dist(samples.reshape(-1, n_particles, 3))\n",
    "    s_hist, s_bins = np.histogram(dist_samples.cpu().numpy(), bins=dist_bins, density=True)\n",
    "    s_hist = s_hist / np.sum(s_hist)\n",
    "\n",
    "\n",
    "    thin_dist = get_dist(data_low[np.random.choice(data_low.shape[0], 5000, replace=False)])\n",
    "    thin_dist_hist, _ = np.histogram(thin_dist.cpu().numpy(), bins=dist_bins, density=True)\n",
    "    thin_dist_hist = thin_dist_hist / np.sum(thin_dist_hist)\n",
    "\n",
    "    plt.hist(dist_samples.cpu().numpy(), bins=dist_bins, density=True, alpha=0.5, label='samples')\n",
    "    plt.hist(thin_dist.cpu().numpy(), bins=dist_bins, density=True, alpha=0.5, label='data')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    TVD = thin_dist_hist - s_hist\n",
    "    TVD = np.abs(TVD)\n",
    "    TVD = np.sum(TVD) / 2\n",
    "    print(f\"Distance TVD: {TVD}\")\n",
    "    Distance_TVD.append(TVD)\n",
    "\n",
    "\n",
    "\n",
    "    # calculate TICA MMD   \n",
    "    xyz = original_samples.detach().cpu().numpy().reshape(-1, 43, 3)  \n",
    "    xyz = xyz[np.random.choice(xyz.shape[0], 5000, replace=False)]          # (T, N, 3)\n",
    "    if target_name == 'a4':\n",
    "        top = md.load('energy/AAAA.pdb').topology\n",
    "    elif target_name == 'a6':\n",
    "        top = md.load('energy/AAAAA.pdb').topology\n",
    "    elif target_name == 'aldp':\n",
    "        top = md.Topology.from_openmm(target.system.topology)\n",
    "    traj = md.Trajectory(xyz, topology=top)\n",
    "    phi_idx,  phi = md.compute_phi(traj)    # radians, shape (T, n_phi)\n",
    "    psi_idx,  psi = md.compute_psi(traj)    # radians, shape (T, n_psi)\n",
    "    feat_tors = np.hstack([phi % (np.pi*2), psi % (np.pi*2)])  # (T, 2*n_phi + 2*n_psi)\n",
    "    ca = [a.index for a in traj.topology.atoms if a.name == 'CA']\n",
    "    pairs = np.array([(i, j) for i in ca for j in ca if j > i], dtype=int)\n",
    "    feat_dists = md.compute_distances(traj, pairs)    # (T, n_pairs), units nm\n",
    "    Y_samples = tica.transform(feat_tors)  # (T, 2)\n",
    "    Y_thin_sample = Y[np.random.choice(Y.shape[0], 5000, replace=False)]\n",
    "\n",
    "    # calculate W2\n",
    "    mm = MMD_fnc(torch.from_numpy(Y_thin_sample).to(device), \n",
    "                                    torch.from_numpy(Y_samples).to(device))\n",
    "    mm  = mm.item()\n",
    "    print(f\"TICA MMD: {mm}\")\n",
    "    TICA_MMD.append(mm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# calculate mean an std\n",
    "print(f'mean and std of Energy_TVD: {np.mean(Energy_TVD):.4f} +- {np.std(Energy_TVD):.4f}')\n",
    "print(f'mean and std of Distance_TVD: {np.mean(Distance_TVD):.4f} +- {np.std(Distance_TVD):.4f}')\n",
    "print(f'mean and std of Sample_W2: {np.mean(Sample_W2):.4f} +- {np.std(Sample_W2):.4f}')\n",
    "print(f'mean and std of TICA_MMD: {np.mean(TICA_MMD):.4f} +- {np.std(TICA_MMD):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FEP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
